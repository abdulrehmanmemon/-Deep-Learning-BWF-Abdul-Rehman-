{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ABDUL REHMAN\n",
        "## E-mail: a.rehmanmemon.034@gmail.com\n",
        "### Date Of Submission: 17/05/2023"
      ],
      "metadata": {
        "id": "w2wnzCwRIPHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ðŸ”´ Task 26\n",
        "\n",
        "Topics: Understanding Optimizers, Last-layer Activations, Loss Functions and Evaluation Metrics\n",
        "\n",
        "Resource: https://drive.google.com/file/d/1i9dPxM_1M4HYN5bYxFcuklC1vM0GrOCq/view?usp=share_link"
      ],
      "metadata": {
        "id": "TGsawmD9IYTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Optimizers**\n",
        "\n",
        "**Optimizers are algorithms that update the weights of a neural network in order to minimize the loss function. Some popular optimizers include:**\n",
        "\n",
        "1. Gradient descent: The most basic optimizer, which updates the weights in the direction of the negative gradient of the loss function.\n",
        "2. Stochastic gradient descent: A variation of gradient descent that updates the weights using a subset of the data, called a mini-batch.\n",
        "3. Momentum: An optimizer that adds a momentum term to the update rule, which helps to prevent the weights from oscillating too much.\n",
        "4. AdaGrad: An optimizer that adapts the learning rate to the gradients of the loss function, which helps to improve convergence.\n",
        "5. RMSProp: An optimizer that is similar to AdaGrad, but it uses a running average of the squared gradients to calculate the learning rate.\n",
        "6. Adam: A popular optimizer that combines the advantages of AdaGrad and RMSProp. \n",
        "\n",
        "#**Last-layer Activations**\n",
        "\n",
        "**The last-layer activation function is a non-linear function that is applied to the output of the neural network before the loss function is calculated. The choice of activation function depends on the type of problem that the neural network is being trained to solve. Some common activation functions include:**\n",
        "\n",
        "1. Sigmoid: A function that outputs a value between 0 and 1, which is often used for classification problems.\n",
        "2. Tanh: A function that outputs a value between -1 and 1, which is often used for regression problems.\n",
        "3. ReLU: A function that outputs the maximum of 0 and its input, which is often used for deep neural networks.\n",
        "4. Leaky ReLU: A variation of ReLU that allows for a small negative output, which can help to prevent the vanishing gradient problem.\n",
        "\n",
        "#**Loss Functions**\n",
        "\n",
        "**The loss function is a measure of the error between the predicted output of the neural network and the ground truth. The choice of loss function depends on the type of problem that the neural network is being trained to solve. Some common loss functions include:**\n",
        "\n",
        "1. Mean squared error (MSE): A loss function that calculates the average squared difference between the predicted output and the ground truth.\n",
        "2. Cross-entropy: A loss function that is used for classification problems, and it calculates the negative log likelihood of the ground truth.\n",
        "3. Mean absolute error (MAE): A loss function that calculates the average absolute difference between the predicted output and the ground truth.\n",
        "\n",
        "#**Evaluation Metrics**\n",
        "\n",
        "**Evaluation metrics are used to measure the performance of a neural network on a held-out test set. Some common evaluation metrics include:**\n",
        "\n",
        "1. Accuracy: The percentage of instances that are correctly classified.\n",
        "2. Precision: The percentage of instances that are correctly classified as positive.\n",
        "3. Recall: The percentage of positive instances that are correctly classified.\n",
        "4. F1 score: A harmonic mean of precision and recall."
      ],
      "metadata": {
        "id": "UGtcPICFIcXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The End ðŸ˜ƒ"
      ],
      "metadata": {
        "id": "fNoySh5ZLfdL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kwpHppLFLoyB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}