{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ABDUL REHMAN\n",
        "## E-mail: a.rehmanmemon.034@gmail.com\n",
        "### Date Of Submission: 08/05/2023"
      ],
      "metadata": {
        "id": "pXsXoKAl9qSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Topics: Underfitting, Overfitting, and Regularization\n",
        "\n",
        "Resource: https://drive.google.com/file/d/1i9dPxM_1M4HYN5bYxFcuklC1vM0GrOCq/view?usp=share_link"
      ],
      "metadata": {
        "id": "-5N4yjqY9u-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##overfitting and underfitting:\n",
        "\n",
        "1. Overfitting happens in every machine-learning problem.\n",
        "2. The tension between optimization and generalization is the fundamental issue in machine learning.\n",
        "3. Optimization refers to adjusting a model to get the best performance possible on the training data.\n",
        "4. Generalization refers to how well the trained model performs on data it has never seen before.\n",
        "5. The goal is to get good generalization.\n",
        "6. At the beginning of training, optimization and generalization are correlated.\n",
        "7. The lower the loss on training data, the lower the loss on test data.\n",
        "8. A model is said to be underfit when optimization and generalization are correlated.\n",
        "9. Generalization stops improving after a certain number of iterations on the training data.\n",
        "10. Validation metrics stall and then begin to degrade when generalization stops improving.\n",
        "11. The model is starting to overfit when this happens.\n",
        "12. Overfitting happens when the model learns patterns that are specific to the training data but that are misleading or irrelevant when it comes to new data.\n",
        "13. To prevent a model from learning misleading or irrelevant patterns found in the training data, the best solution is to get more training data.\n",
        "14. The next-best solution is to modulate the quantity of information that your model is allowed to store or to add constraints on what information it’s allowed to store.\n",
        "15. The processing of fighting overfitting this way is called regularization.\n",
        "16. The simplest way to prevent overfitting is to reduce the size of the model.\n",
        "17. The number of learnable parameters in a model is often referred to as the model’s capacity.\n",
        "18. A model with more parameters has more memorization capacity and therefore can easily learn a perfect dictionary-like mapping between training samples and their targets without any generalization power.\n",
        "19. On the other hand, if the network has limited memorization resources, it won’t be able to learn this mapping as easily.\n",
        "20. You should use models that have enough parameters that they don’t underfit."
      ],
      "metadata": {
        "id": "D9akY08G95Uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regularization\n",
        "\n",
        "1) Regularization is a way to prevent overfitting in neural networks.\n",
        "\n",
        "2) Simpler models are less likely to overfit than complex ones.\n",
        "\n",
        "3) Weight regularization adds constraints on the complexity of a network by forcing its weights to take only small values, which makes the distribution of weight values more regular.\n",
        "\n",
        "4) L1 regularization and L2 regularization are two ways to add a cost associated with having large weights.\n",
        "\n",
        "5) L2 regularization is also called weight decay in the context of neural networks.\n",
        "\n",
        "6) In Keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments.\n",
        "\n",
        "7) L2 weight regularization penalty adds a value proportional to the square of the value of the weight coefficients.\n",
        "\n",
        "8) Loss for a network with regularization penalty added will be much higher at training time than at test time.\n",
        "\n",
        "9) Dropout is one of the most effective and most commonly used regularization techniques for neural networks.\n",
        "\n",
        "10) Dropout is applied to a layer by randomly dropping out a number of output features of the layer during training.\n",
        "\n",
        "11) Dropout rate is the fraction of the features that are zeroed out.\n",
        "\n",
        "12) At test time, no units are dropped out; instead, the layer’s output values are scaled down by a factor equal to the dropout rate.\n",
        "\n",
        "13) Dropout can be implemented by doing both operations at training time and leaving the output unchanged at test time.\n",
        "\n",
        "14) Dropout helps reduce overfitting by preventing complex co-adaptations on training data.\n",
        "\n",
        "15) Hinton was inspired by a technique used by his colleague that involved dropping out units in the model to reduce overfitting.\n",
        "\n",
        "16) Dropout reduces complex co-adaptations on training data by forcing the network to learn redundancies in the data.\n",
        "\n",
        "17) Dropout helps the network generalize better to new data.\n",
        "\n",
        "18) A downside of dropout is that it increases the time needed to train the network.\n",
        "\n",
        "19) Dropout can be added to a model using the Dropout layer in Keras.\n",
        "\n",
        "20) The Dropout layer in Keras takes the dropout rate as its argument."
      ],
      "metadata": {
        "id": "T8RrRAYB-WaC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G4rMMnwO-w8T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}