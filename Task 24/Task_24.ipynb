{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ABDUL REHMAN\n",
        "## E-mail: a.rehmanmemon.034@gmail.com\n",
        "### Date Of Submission: 08/05/2023"
      ],
      "metadata": {
        "id": "89CRYnO87UU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Topics: Data Preprocessing and Feature Engineering\n",
        "\n",
        "Resource: https://drive.google.com/file/d/1i9dPxM_1M4HYN5bYxFcuklC1vM0GrOCq/view?usp=share_link"
      ],
      "metadata": {
        "id": "DvAApSZl7Z3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing, Feature Engineering, and Feature Learning\n",
        "\n",
        "1. Before feeding data into a neural network, it needs to be prepared through data preprocessing.\n",
        "2. Data preprocessing is domain-specific and includes techniques like normalization, feature extraction, handling missing values, etc.\n",
        "3. Vectorization is a necessary step in data preprocessing, where all inputs and targets must be tensors of floating-point data.\n",
        "4. Data vectorization involves turning the raw data into tensors, regardless of the type of data, like sound, images, text.\n",
        "5. One-hot encoding is used to convert text data into a tensor of float32 data.\n",
        "6. Value normalization involves converting the data into small values, typically in the range of 0-1, and homogenous to converge the network quickly.\n",
        "7. Normalizing each feature independently is a common and stricter normalization practice, which involves normalizing each feature to have a mean of 0 and standard deviation of 1.\n",
        "8. Data with relatively large values or data that is heterogeneous can trigger large gradient updates, making learning harder.\n",
        "9. Handling missing values is an important aspect of data preprocessing.\n",
        "10. It is safe to input missing values as 0, provided 0 is not already a meaningful value in the data.\n",
        "11. Neural networks learn from exposure to data that the value 0 denotes missing data.\n",
        "12. Feature engineering involves selecting and transforming relevant features that improve the model's performance.\n",
        "13. Feature engineering is a domain-specific task that requires expert knowledge.\n",
        "14. Feature extraction is a common technique used in feature engineering that involves transforming raw data into meaningful representations.\n",
        "15. Autoencoders are neural networks that learn to encode data into lower-dimensional representations for efficient feature extraction.\n",
        "16. Convolutional neural networks (CNNs) are commonly used for image feature extraction.\n",
        "17. Recurrent neural networks (RNNs) are used for sequential data feature extraction.\n",
        "18. Transfer learning involves using pre-trained neural networks as a feature extractor for a different task.\n",
        "19. Fine-tuning involves training a pre-trained neural network with new data to adapt it to a new task.\n",
        "20. Feature learning is an alternative to feature engineering that involves letting the neural network learn the relevant features on its own.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0OR3EEwM8P5K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FSF-vmIe9S3P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}