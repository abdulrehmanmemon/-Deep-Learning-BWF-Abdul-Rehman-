{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#ABDUL REHMAN\n",
        "#E-mail: a.rehmanmemon.034@gmail.com\n",
        "#Date Of Submission: 02/07/2023"
      ],
      "metadata": {
        "id": "wFWpDfyfiBj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”´ Task 30\n",
        "\n",
        "# Topics: Recurrent Neural Network, LSTM, GRU"
      ],
      "metadata": {
        "id": "3roPO8kRiHdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ðŸ‘‰ Recurrent Neural Networks (RNNs):\n",
        "1. RNNs are a class of neural networks designed to process sequential data.\n",
        "\n",
        "2. They have recurrent connections that allow information to persist and be shared across different time steps.\n",
        "\n",
        "3. RNNs are suitable for tasks like speech recognition, natural language processing, and time series analysis.\n",
        "\n",
        "4. RNNs suffer from the vanishing gradient problem, which makes it difficult for them to capture long-term dependencies."
      ],
      "metadata": {
        "id": "EIKb0IAdicS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ðŸ‘‰ Long Short-Term Memory (LSTM):\n",
        "\n",
        "1. LSTMs are designed to mitigate the vanishing gradient problem by using a more sophisticated gating mechanism.\n",
        "2. They can capture and preserve long-term dependencies in sequential data by selectively updating and accessing the cell state.\n",
        "3. LSTMs have an internal memory cell that can store and retrieve information over long sequences, allowing them to maintain information over extended time intervals.\n",
        "4. They are particularly effective in tasks that involve long-range dependencies, such as speech recognition, handwriting recognition, and language modeling.\n",
        "5. LSTMs have a larger number of parameters compared to standard RNNs, which gives them more expressive power but also requires more computational resources for training and inference."
      ],
      "metadata": {
        "id": "bOmT7f7zjIAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ðŸ‘‰ Gated Recurrent Unit (GRU):\n",
        "1. GRUs are a simplified variant of LSTMs that aim to achieve similar results with a smaller number of parameters.\n",
        "2. They merge the cell state and hidden state into a single state vector, simplifying the architecture compared to LSTMs.\n",
        "3. GRUs have two gates: an update gate and a reset gate. The update gate controls the balance between old and new information, while the reset gate determines how much past information is forgotten.\n",
        "4. GRUs have been shown to be computationally efficient and can yield comparable performance to LSTMs on many tasks.\n",
        "5. They are useful in scenarios where memory footprint or computational resources are limited, making them popular in mobile and embedded systems.\n",
        "6. GRUs have been successfully applied in various domains, including machine translation, image captioning, and sentiment analysis."
      ],
      "metadata": {
        "id": "-yh9PU2ajd_S"
      }
    }
  ]
}